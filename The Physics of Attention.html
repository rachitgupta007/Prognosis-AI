<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Physics of Attention | Rachit Gupta</title>
    
    <!-- Fonts: Newsreader (Serif) & JetBrains Mono (Technical) -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500;700&family=Newsreader:ital,opsz,wght@0,6..72,200..800;1,6..72,200..800&display=swap" rel="stylesheet">
    
    <!-- MathJax for LaTeX Rendering -->
    <script>
        window.MathJax = {
            tex: { inlineMath: [['$', '$'], ['\\(', '\\)']] },
            svg: { fontCache: 'global' }
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <style>
        /* --- DESIGN SYSTEM --- */
        :root {
            --c-paper: #F2F0E6;
            --c-ink: #1A1C1B;
            --c-ink-soft: #4A4D4B;
            --c-accent: #D64000; /* International Orange */
            --c-grid: #D1CEC4;
            
            /* Visualization Semantic Colors */
            --c-success: #2E7D32; /* SRAM / Efficient */
            --c-danger: #C62828;  /* HBM / Stall */
            --c-warn: #F57F17;    /* Compute / ALU */

            --f-serif: 'Newsreader', serif;
            --f-mono: 'JetBrains Mono', monospace;

            --easing-sharp: cubic-bezier(0.76, 0, 0.24, 1);
        }

        * { margin: 0; padding: 0; box-sizing: border-box; }

        html, body {
            background-color: var(--c-paper);
            color: var(--c-ink);
            font-family: var(--f-serif);
            font-size: 20px;
            line-height: 1.75;
            overflow-x: hidden;
            scroll-behavior: smooth;
            -webkit-font-smoothing: antialiased;
        }

        /* --- LAYOUT --- */
        .container {
            max-width: 820px;
            margin: 0 auto;
            padding: 140px 24px 120px;
        }

        /* --- ANIMATIONS --- */
        @keyframes fadeInUp {
            from { opacity: 0; transform: translateY(20px); }
            to { opacity: 1; transform: translateY(0); }
        }
        .animate-enter { animation: fadeInUp 0.8s var(--easing-sharp) forwards; opacity: 0; }
        .delay-1 { animation-delay: 0.1s; }
        .delay-2 { animation-delay: 0.2s; }
        .delay-3 { animation-delay: 0.3s; }

        /* --- TYPOGRAPHY --- */
        h1, h2, h3, h4 {
            font-family: var(--f-serif);
            font-weight: 700;
            color: var(--c-ink);
            letter-spacing: -0.02em;
        }

        h1 { font-size: 3.5rem; line-height: 1.1; margin-bottom: 0.5rem; font-weight: 800; }
        h2 { font-size: 2rem; margin-top: 3.5rem; margin-bottom: 1.5rem; border-top: 1px solid var(--c-grid); padding-top: 1.5rem; }
        h3 { font-size: 1.2rem; margin-top: 2rem; margin-bottom: 0.75rem; font-family: var(--f-mono); text-transform: uppercase; letter-spacing: 0.05em; color: var(--c-ink-soft); }

        p { margin-bottom: 1.5rem; color: var(--c-ink); font-weight: 400; }
        strong { font-weight: 700; color: var(--c-ink); }
        
        /* Inline Code */
        code {
            font-family: var(--f-mono);
            font-size: 0.85em;
            background: rgba(0,0,0,0.04);
            padding: 2px 4px;
            border-radius: 3px;
            color: var(--c-danger);
        }

        /* Blockquote */
        blockquote {
            border-left: 3px solid var(--c-accent);
            padding-left: 1.5rem;
            margin: 2.5rem 0;
            font-style: italic;
            color: var(--c-ink-soft);
            background: rgba(0,0,0,0.02);
            padding: 1.5rem;
        }

        /* --- NAVIGATION --- */
        nav {
            display: flex; justify-content: space-between; align-items: center;
            padding: 1.5rem 3rem; border-bottom: 1px solid rgba(26, 28, 27, 0.05);
            position: fixed; top: 0; width: 100%;
            background: rgba(242, 240, 230, 0.95); backdrop-filter: blur(10px); z-index: 100;
        }
        .logo { font-weight: 700; font-size: 1.2rem; letter-spacing: -0.03em; font-family: var(--f-mono); }
        .nav-links { display: flex; gap: 2.5rem; }
        .nav-links a {
            font-family: var(--f-mono); font-size: 0.8rem; text-transform: uppercase;
            opacity: 0.6; border: none; text-decoration: none; color: inherit;
        }
        .nav-links a:hover, .nav-links a.active { opacity: 1; color: var(--c-accent); }

        /* --- CITATIONS & POPOVERS --- */
        .citation {
            vertical-align: super; font-size: 0.7em; color: var(--c-accent);
            cursor: pointer; margin-left: 2px; font-family: var(--f-mono); position: relative;
        }
        
        .popover {
            position: absolute; background: var(--c-ink); color: var(--c-paper);
            padding: 1rem; border-radius: 2px; width: 320px;
            font-size: 0.8rem; font-family: var(--f-mono); z-index: 1000;
            box-shadow: 0 10px 40px rgba(0, 0, 0, 0.2); opacity: 0; pointer-events: none;
            transition: opacity 0.2s, transform 0.2s; transform: translateY(10px) translateX(-50%);
            left: 50%; bottom: 140%; text-align: left; line-height: 1.4;
        }
        .popover::after {
            content: ''; position: absolute; bottom: -6px; left: 50%; margin-left: -6px;
            border-width: 6px 6px 0; border-style: solid; border-color: var(--c-ink) transparent transparent transparent;
        }
        .citation:hover .popover { opacity: 1; pointer-events: auto; transform: translateY(0) translateX(-50%); }

        /* --- VISUALIZATIONS --- */
        .viz-wrapper {
            border: 1px solid var(--c-grid);
            background: #FAFAFA;
            padding: 2rem;
            border-radius: 4px;
            margin: 3.5rem 0 2rem 0;
            box-shadow: 0 4px 30px rgba(0,0,0,0.04);
            position: relative;
        }
        
        .viz-header {
            display: flex; justify-content: space-between; border-bottom: 1px solid var(--c-grid);
            padding-bottom: 0.8rem; margin-bottom: 1.5rem; align-items: center;
        }
        .viz-title { font-family: var(--f-mono); font-weight: 700; font-size: 0.8rem; text-transform: uppercase; color: var(--c-ink-soft); }

        .viz-controls { display: flex; justify-content: center; gap: 1rem; margin-top: 1.5rem; }
        .viz-btn {
            background: transparent; border: 1px solid var(--c-grid); padding: 0.6rem 1.2rem;
            font-family: var(--f-mono); font-size: 0.8rem; cursor: pointer; color: var(--c-ink-soft);
            transition: all 0.2s;
        }
        .viz-btn:hover { border-color: var(--c-ink); color: var(--c-ink); }
        .viz-btn.active { background: var(--c-ink); color: var(--c-paper); border-color: var(--c-ink); }

        /* VIZ 1: Bandwidth Bars */
        .bw-row { display: grid; grid-template-columns: 100px 1fr 100px; gap: 1rem; align-items: center; margin-bottom: 1rem; }
        .bw-label { font-family: var(--f-mono); font-size: 0.8rem; text-align: right; }
        .bw-track { height: 32px; background: #eee; width: 100%; border-radius: 2px; position: relative; overflow: hidden; }
        .bw-fill { height: 100%; width: 0%; transition: width 1s cubic-bezier(0.2, 0.8, 0.2, 1); display: flex; align-items: center; justify-content: flex-end; padding-right: 10px; color: #fff; font-family: var(--f-mono); font-size: 0.75rem; }
        .bw-val { font-family: var(--f-mono); font-size: 0.8rem; color: var(--c-ink-soft); }
        .fill-sram { background: var(--c-success); }
        .fill-hbm { background: var(--c-danger); }

        /* VIZ 2: Tiling Math */
        .tiling-layout { display: flex; gap: 2rem; flex-wrap: wrap; justify-content: center; align-items: flex-start; }
        .grid-container {
            display: grid; grid-template-columns: repeat(8, 1fr); gap: 2px;
            width: 260px; height: 260px; margin: 0 auto;
        }
        .grid-cell { background: #eee; position: relative; transition: background 0.1s; }
        .grid-cell.active { background: var(--c-success); z-index: 10; box-shadow: 0 0 15px rgba(46,125,50,0.4); transform: scale(1.1); border: 1px solid #fff; }
        .grid-cell.processed { background: #dcedc8; }
        
        .math-panel {
            flex: 1; min-width: 280px; background: #fff; border: 1px solid var(--c-grid);
            padding: 1.5rem; font-family: var(--f-mono); font-size: 0.85rem;
        }
        .math-row { display: flex; justify-content: space-between; margin-bottom: 0.8rem; padding-bottom: 0.5rem; border-bottom: 1px dashed #eee; }
        .math-label { color: var(--c-ink-soft); }
        .math-val { font-weight: 700; color: var(--c-accent); transition: color 0.3s; }
        .math-val.update { color: var(--c-success); }

        /* VIZ 3: Pipeline Gantt */
        canvas.pipeline-canvas { width: 100%; height: 220px; background: #fafafa; border: 1px solid #eee; }

        /* --- FOOTER --- */
        footer {
            padding: 6rem 0; border-top: 1px solid var(--c-grid);
            display: flex; justify-content: space-between;
            font-family: var(--f-mono); font-size: 0.85rem;
            color: var(--c-ink-soft); margin-top: 6rem;
        }
        .footer-links a { margin-left: 2rem; opacity: 0.6; }
        .footer-links a:hover { opacity: 1; color: var(--c-accent); }
    </style>
</head>
<body>

    <nav>
        <div class="logo">RACHIT GUPTA</div>
        <div class="nav-links">
            <a href="#" class="active">Articles</a>
            <a href="#">About</a>
            <a href="#">Research</a>
        </div>
    </nav>

    <div class="container animate-enter">
        
        <header>
            <div class="mono caps" style="color: var(--c-accent); margin-bottom: 1rem;">System Architecture</div>
            <h1>The Physics of Attention</h1>
            <div class="mono" style="margin-top: 1rem; color: var(--c-ink-soft); font-size: 1.1rem;">
                FlashAttention, IO-Awareness, and the Memory Wall
            </div>
            <div class="mono" style="margin-top: 0.5rem; font-size: 0.8rem; opacity: 0.7;">
                November 2025 • 25 min read • By Rachit Gupta
            </div>
        </header>

        <section style="margin-top: 4rem;">
            <p>
                Hi everyone. Today we are going to perform open-heart surgery on the Transformer architecture.
            </p>
            <p>
                In the Deep Learning community, we often exist in a world of pure mathematical abstraction. We visualize Neural Networks as clean, Directed Acyclic Graphs (DAGs) of floating-point operations. We analyze the $O(N^2)$ complexity of Attention, nod wisely, and assume we understand why long contexts are difficult. The assumption is seductive: if the FLOPs (Floating Point Operations) are manageable, the model should train efficiently.
            </p>
            <p>
                Here is the uncomfortable reality of Large Language Model training in 2025: <strong>The math is rarely the bottleneck.</strong>
            </p>
            <p>
                If you profile a training run with a context window of 128k or 1M tokens, you will witness something terrifying in the GPU metrics. The Tensor Cores—massive silicon engines designed to crush $4 \times 4$ matrix blocks at hundreds of TFLOPS—are often silent. They are stalling.
            </p>
            <p>
                Why? Because they are waiting for data. They are victims of the <strong>Memory Wall</strong>.
            </p>
            <p>
                In this post, we are peeling back the PyTorch abstraction layer. We are going down to the "bare metal" to discuss the <em>physics</em> of how electrons move through silicon. We will explore <strong>FlashAttention</strong><span class="citation">[1]<span class="popover">Dao, T. et al. (2022). "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness". NeurIPS.</span></span>, the concept of <strong>IO-Awareness</strong>, and why reading from HBM is the ultimate enemy of speed.
            </p>
        </section>

        <!-- SECTION 1 -->
        <h2>1. The Hardware Reality: Bandwidth vs. Latency</h2>
        <section>
            <p>
                To understand FlashAttention, you must understand the machine you are programming. When you execute <code>model.to('cuda')</code>, you aren't placing data into a uniform bucket. You are navigating a complex hierarchy of memory warehouses.
            </p>
            <p>
                The fundamental tension in GPU architecture is between <strong>Compute</strong> (crunching numbers) and <strong>Memory Access</strong> (fetching numbers). Let's examine the NVIDIA A100 (80GB), the hardware that defined the early 2020s AI boom.
            </p>

            <div class="viz-wrapper">
                <div class="viz-header">
                    <span class="viz-title">Hardware Physics Simulator</span>
                    <span class="viz-title mono" style="color: var(--c-accent)">A100 SPECS</span>
                </div>
                
                <div class="bw-row">
                    <div class="bw-label">HBM<br><span style="font-size:0.7em; opacity:0.6">Main Memory</span></div>
                    <div class="bw-track"><div class="bw-fill fill-hbm" id="bar-hbm"></div></div>
                    <div class="bw-val">1.9 TB/s</div>
                </div>
                
                <div class="bw-row">
                    <div class="bw-label">SRAM<br><span style="font-size:0.7em; opacity:0.6">L1 Cache</span></div>
                    <div class="bw-track"><div class="bw-fill fill-sram" id="bar-sram"></div></div>
                    <div class="bw-val">19.0 TB/s</div>
                </div>

                <div class="viz-controls">
                    <button class="viz-btn" onclick="runBandwidthSim()">Run Throughput Sim</button>
                </div>
                <div class="mono" style="font-size: 0.75rem; text-align: center; margin-top: 1rem; color: var(--c-ink-soft);">
                    <strong>Physics:</strong> The SRAM (Green) is 10x faster than HBM (Red).<br>If your data is in HBM, your GPU core is waiting 90% of the time.
                </div>
            </div>

            <h3>The Chef and The Pantry</h3>
            <p>
                I use a specific mental model to visualize this hierarchy: "The Chef and The Pantry."
            </p>
            <ul>
                <li><strong>The Chef (Tensor Cores):</strong> An entity capable of chopping ingredients at light speed (312 TFLOPS).</li>
                <li><strong>The Kitchen Counter (SRAM):</strong> A tiny workspace (192KB per core) immediately accessible to the Chef. Bandwidth: ~19 TB/s.</li>
                <li><strong>The Pantry (HBM):</strong> A massive warehouse down the hall (80GB). Bandwidth: ~1.9 TB/s.</li>
            </ul>
            <p>
                Standard algorithms treat the GPU as a math engine. They send the Chef to the Pantry to fetch a single carrot, walk back, chop it, and then walk back for an onion. The Chef spends 99% of the time walking (Memory Latency) and 1% chopping.
            </p>
            <p>
                <strong>IO-Awareness</strong> is the realization that the algorithm must account for the "walk." The Chef should take a crate to the pantry, fill it with a block of ingredients (Tiling), and refuse to leave the counter until that entire block is processed. This approach maximizes <strong>Arithmetic Intensity</strong>—the ratio of floating point operations performed per byte of data moved.
            </p>
        </section>

        <!-- SECTION 2 -->
        <h2>2. The Villain: Materializing the $N^2$ Matrix</h2>
        <section>
            <p>
                Let's rigorously analyze the standard Attention mechanism. The equation is elegant:
            </p>
            $$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V $$
            <p>
                However, implementation details are violent. In a naive PyTorch implementation, the execution happens strictly step-by-step, materializing intermediate matrices to HBM at every stage.
            </p>
            <p>Let's trace the memory movement for sequence length $N$:</p>
            <ol>
                <li><strong>Matmul 1:</strong> Read $Q, K$. Compute $S = QK^T$. <strong>Write $S$ ($N \times N$) to HBM.</strong></li>
                <li><strong>Softmax:</strong> Read $S$. Compute $P = \text{softmax}(S)$. <strong>Write $P$ ($N \times N$) to HBM.</strong></li>
                <li><strong>Matmul 2:</strong> Read $P, V$. Compute $O = PV$. Write $O$ to HBM.</li>
            </ol>
            <p>
                For a sequence length $N = 32,000$, the matrix $S$ contains $10^9$ elements. In FP16 (2 bytes), that is <strong>2GB</strong> of data. We write 2GB, read 2GB, write 2GB, read 2GB. That is 8GB of memory traffic for a single layer's single head. We are thrashing the HBM bandwidth to store a matrix that we don't strictly need to keep.
            </p>
        </section>

        <!-- SECTION 3 -->
        <h2>3. FlashAttention: Tiling & The Softmax Trick</h2>
        <section>
            <p>
                FlashAttention fuses these operations. It loads blocks of $Q, K, V$ into SRAM, computes the attention output locally, and writes only the final result to HBM. The $N \times N$ matrix is never fully materialized.
            </p>
            <p>
                The challenge is the <strong>Softmax coupling</strong>.
            </p>
            $$ \text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}} $$
            <p>
                To compute the probability for token $i$, you need the denominator (sum of exponentials) for <em>all</em> tokens $j$. If we are processing in blocks, we only see a subset of the row at a time. We don't know the global denominator!
            </p>
            
            <h3>The Solution: Online Softmax</h3>
            <p>
                We use a streaming statistic update trick. We track a running max ($m$) and a running sum of exponentials ($\ell$). When we process a new block with local max $m_{block}$, we update our global stats and "retroactively correct" our partial output $O$ by rescaling it:
            </p>
            $$ O_{new} = \text{diag}(\ell_{old} / \ell_{new}) e^{m_{old} - m_{new}} O_{old} + \dots $$
            
            <p>This equation allows us to fuse the MatMul, the Masking, the Softmax, and the second MatMul into a single CUDA kernel.</p>

            <div class="viz-wrapper">
                <div class="viz-header">
                    <span class="viz-title">Tiling & Online Statistics Visualizer</span>
                    <span class="viz-title mono" id="tile-status">STATUS: IDLE</span>
                </div>
                
                <div class="tiling-layout">
                    <div>
                        <div class="mono" style="font-size:0.7em; margin-bottom:5px; text-align:center;">VIRTUAL ATTENTION MATRIX</div>
                        <div class="grid-container" id="grid-viz"></div>
                    </div>
                    
                    <div class="math-panel">
                        <div class="mono caps" style="font-size:0.75rem; color:var(--c-ink-soft); margin-bottom:1rem;">SRAM Registers (Per Row)</div>
                        
                        <div class="math-row">
                            <span class="math-label">Block Max ($m_j$):</span>
                            <span class="math-val" id="val-block">--</span>
                        </div>
                        <div class="math-row">
                            <span class="math-label">Global Max ($m$):</span>
                            <span class="math-val" id="val-m">0.00</span>
                        </div>
                        <div class="math-row">
                            <span class="math-label">Running Sum ($\ell$):</span>
                            <span class="math-val" id="val-l">0.00</span>
                        </div>
                        
                        <div class="mono" style="font-size: 0.75rem; margin-top: auto; color: var(--c-ink-soft); line-height: 1.4;">
                            *Notice: When a new Block Max exceeds Global Max, the Running Sum and Output are mathematically rescaled.
                        </div>
                    </div>
                </div>
                <div class="viz-controls">
                    <button class="viz-btn" id="btn-tiling" onclick="toggleTiling()">Run Simulation</button>
                </div>
            </div>
        </section>

        <!-- SECTION 4 -->
        <h2>4. FlashAttention-2 & Warp Specialization</h2>
        <section>
            <p>
                While FA1 was a massive improvement, it was still leaving performance on the table. In 2023, <strong>FlashAttention-2</strong><span class="citation">[2]<span class="popover">Dao, T. (2023). "FlashAttention-2: Faster Attention with Better Parallelism". ICLR.</span></span> introduced "Warp Specialization" using the NVIDIA <strong>Cutlass</strong> library.
            </p>
            <p>
                The GPU executes threads in bundles of 32 called <strong>Warps</strong>. In FA1, all warps did everything: Load $\to$ Compute. This creates "pipeline bubbles"—while waiting for data, the math cores sleep.
            </p>
            <p>
                FA2 splits the warps:
            </p>
            <ul>
                <li><strong>Producer Warps:</strong> Sole responsibility is fetching data from HBM to SRAM.</li>
                <li><strong>Consumer Warps:</strong> Sole responsibility is Math (Tensor Core GEMMs).</li>
            </ul>
            <p>
                This allows <em>asynchronous</em> memory access. The Producer fetches Block $i+1$ while the Consumer crunches Block $i$.
            </p>

            <div class="viz-wrapper">
                <div class="viz-header">
                    <span class="viz-title">Instruction Pipeline: Latency Hiding</span>
                    <span class="viz-title mono">GANTT CHART</span>
                </div>
                <canvas id="pipelineCanvas" class="pipeline-canvas"></canvas>
                <div class="viz-controls">
                    <button class="viz-btn active" onclick="setPipelineMode('naive')">Naive (Stall)</button>
                    <button class="viz-btn" onclick="setPipelineMode('pipelined')">Flash-2 (Async)</button>
                </div>
                <div class="mono" style="font-size: 0.75rem; text-align: center; margin-top: 1rem; color: var(--c-ink-soft);">
                    <strong style="color:var(--c-danger)">RED</strong> = Memory Load (HBM) | <strong style="color:var(--c-success)">GREEN</strong> = Compute (Math).<br>
                    Notice how Async Pipelining overlaps the Red and Green blocks, hiding the memory latency.
                </div>
            </div>
        </section>

        <!-- SECTION 5 -->
        <h2>5. Beyond $O(N^2)$: Mamba & Linear Attention</h2>
        <section>
            <p>
                FlashAttention optimizes the constants, but the complexity remains quadratic. Doubling context quadruples the work. For truly massive contexts (1M+ tokens), we need to break the law of physics.
            </p>
            <p>
                Architectures like <strong>Mamba</strong> (State Space Models) offer a Linear Attention mechanism ($O(N)$). They achieve this by compressing the past into a fixed-size state $h_t$, rather than keeping the full history cache $KV$.
            </p>
            $$ h_t = A h_{t-1} + B x_t $$
            <p>
                However, even Mamba relies heavily on the principles of <strong>IO-Awareness</strong>. It uses a "Parallel Scan" algorithm that is carefully tiled to fit into SRAM, minimizing HBM trips. The lesson remains: The architecture matters less than the data movement.
            </p>
        </section>

        <footer>
            <div style="display: flex; flex-direction: column; gap: 0.5rem;">
                <strong>Rachit Gupta</strong>
                <span>Research Scientist & Engineer</span>
                <span style="opacity: 0.5;">© 2025</span>
            </div>
            <div class="footer-links">
                <a href="#">Twitter</a>
                <a href="#">Github</a>
                <a href="#">Scholar</a>
            </div>
        </footer>

    </div>

    <!-- JAVASCRIPT LOGIC -->
    <script>
        // --- 1. BANDWIDTH SIMULATOR ---
        function runBandwidthSim() {
            const hbm = document.getElementById('bar-hbm');
            const sram = document.getElementById('bar-sram');
            
            // Reset
            hbm.style.width = '0%';
            sram.style.width = '0%';
            hbm.style.transition = 'none';
            sram.style.transition = 'none';
            
            setTimeout(() => {
                // SRAM: 19 TB/s -> Extremely fast visual (0.2s)
                sram.style.transition = 'width 0.2s cubic-bezier(0.1, 0.7, 1.0, 0.1)';
                sram.style.width = '100%';
                
                // HBM: 1.9 TB/s -> 10x slower visual (2.5s)
                hbm.style.transition = 'width 2.5s linear';
                hbm.style.width = '100%';
            }, 100);
        }
        // Auto-run once
        setTimeout(runBandwidthSim, 1000);


        // --- 2. TILING & MATH ENGINE ---
        const gridViz = document.getElementById('grid-viz');
        const tileStatus = document.getElementById('tile-status');
        const valBlock = document.getElementById('val-block');
        const valM = document.getElementById('val-m');
        const valL = document.getElementById('val-l');
        
        const N = 8; 
        const cells = [];
        
        // Build Grid
        for(let i=0; i<N*N; i++) {
            let div = document.createElement('div');
            div.className = 'grid-cell';
            gridViz.appendChild(div);
            cells.push(div);
        }

        let isTiling = false;
        let tileInterval;
        let q = 0, k = 0;
        let runningMax = 0;
        let runningSum = 0;

        function toggleTiling() {
            const btn = document.getElementById('btn-tiling');
            if(isTiling) {
                clearInterval(tileInterval);
                isTiling = false;
                btn.innerText = "Resume Simulation";
                tileStatus.innerText = "PAUSED";
                return;
            }
            
            isTiling = true;
            btn.innerText = "Pause Simulation";
            
            tileInterval = setInterval(() => {
                // Clear active
                cells.forEach(c => c.classList.remove('active'));
                
                let idx = q * N + k;
                
                if(idx < cells.length) {
                    const cell = cells[idx];
                    cell.classList.add('active');
                    cell.classList.add('processed');
                    
                    // --- MATH LOGIC ---
                    let blockMax = (Math.random() * 2).toFixed(2);
                    valBlock.innerText = blockMax;
                    
                    let oldMax = runningMax;
                    let newMax = Math.max(parseFloat(oldMax), parseFloat(blockMax));
                    
                    // Flash the value if updated
                    if(newMax > parseFloat(oldMax)) {
                        valM.classList.add('update');
                        setTimeout(() => valM.classList.remove('update'), 300);
                    }
                    runningMax = newMax.toFixed(2);
                    
                    // Dummy sum logic for visualization
                    let addedExp = Math.exp(blockMax - newMax);
                    runningSum = (parseFloat(runningSum) + addedExp).toFixed(2);
                    
                    valM.innerText = runningMax;
                    valL.innerText = runningSum;
                    
                    tileStatus.innerText = `COMPUTING: Q[${q}] K[${k}]`;
                }
                
                // --- LOOP LOGIC ---
                k++;
                if(k >= N) {
                    k = 0;
                    q++;
                    // Reset stats for new row (Q)
                    runningMax = 0; runningSum = 0;
                    if(q >= N) {
                        q = 0;
                        // Reset grid visuals
                        cells.forEach(c => c.classList.remove('processed'));
                    }
                }
            }, 150);
        }


        // --- 3. PIPELINE GANTT CHART ---
        const canvas = document.getElementById('pipelineCanvas');
        const ctx = canvas.getContext('2d');
        let pipelineMode = 'naive';
        let frame = 0;
        let tasks = [];

        // Retina Display Fix
        const dpr = window.devicePixelRatio || 1;
        const rect = canvas.getBoundingClientRect();
        canvas.width = rect.width * dpr;
        canvas.height = rect.height * dpr;
        ctx.scale(dpr, dpr);
        const W = rect.width;
        const H = rect.height;

        function setPipelineMode(mode) {
            pipelineMode = mode;
            tasks = [];
            frame = 0;
            // Toggle Buttons
            const btns = document.querySelectorAll('.pipeline-canvas ~ .viz-controls .viz-btn');
            btns.forEach(b => b.classList.remove('active'));
            event.target.classList.add('active');
        }

        function drawPipeline() {
            // Background
            ctx.fillStyle = '#fafafa';
            ctx.fillRect(0,0, W, H);
            
            // Lanes
            ctx.strokeStyle = '#eee';
            ctx.lineWidth = 1;
            ctx.beginPath();
            ctx.moveTo(0, H/3); ctx.lineTo(W, H/3);
            ctx.moveTo(0, H*2/3); ctx.lineTo(W, H*2/3);
            ctx.stroke();

            // Labels
            ctx.fillStyle = '#999';
            ctx.font = '10px JetBrains Mono';
            ctx.fillText("MEMORY (HBM)", 10, H/3 - 10);
            ctx.fillText("COMPUTE (SRAM)", 10, H*2/3 - 10);

            // Generate Tasks
            // In Naive mode, we need large gaps. In Pipeline, small gaps.
            let freq = pipelineMode === 'naive' ? 120 : 70;
            if(frame % freq === 0) {
                tasks.push({ x: W, width: 60, id: frame });
            }

            // Draw Tasks
            tasks.forEach(t => {
                t.x -= 2; // Move left
                
                // Memory Block (Red)
                ctx.fillStyle = '#c62828';
                ctx.fillRect(t.x, 30, t.width, 30);
                ctx.fillStyle = '#fff';
                ctx.fillText("LOAD", t.x+15, 50);
                
                // Compute Block (Green)
                let computeX;
                if(pipelineMode === 'naive') {
                    // Serial: Compute starts after Load finishes
                    computeX = t.x + t.width + 5; 
                } else {
                    // Pipelined: Compute starts while next Load is happening
                    // Visually, we just offset it slightly to show overlap
                    computeX = t.x + 20; 
                }
                
                ctx.fillStyle = '#2e7d32';
                ctx.fillRect(computeX, H/3 + 40, t.width, 30);
                ctx.fillStyle = '#fff';
                ctx.fillText("MATH", computeX+15, H/3 + 60);
                
                // Connecting Arrow (visual fluff)
                if(pipelineMode === 'naive') {
                    ctx.strokeStyle = '#ddd';
                    ctx.beginPath();
                    ctx.moveTo(t.x + t.width, 45);
                    ctx.lineTo(computeX, H/3 + 55);
                    ctx.stroke();
                }
            });

            // Cleanup
            tasks = tasks.filter(t => t.x > -150);
            frame++;
            requestAnimationFrame(drawPipeline);
        }
        drawPipeline();

    </script>
</body>
</html>